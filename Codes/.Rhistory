Ccurr <- Gjh + 2*j - 1 -(j-1)^2/Cprev
Dcurr <- 1/Dcurr
Delta <- Ccurr * Dcurr
fcurr <- fprev*Delta
fprev <- fcurr
Cprev <- Ccurr
Dprev <- Dcurr
if(abs(Delta - 1)< err2){
STOP <- TRUE
}
}
return(1/(Gjh + 1 + fcurr))
}
Lentz_Q(1.1)
exp(1.1)*expint_E1(1.1)
Lentz_Q(1.2)
exp(1.2)*expint_E1(1.2)
Lentz_Q <- function(Gjh){
b0 <- 1 +Gjh
err1 <- 10^(-30)
err2 <- 10^(-7)
fprev <- err1
Cprev <- err1
Dprev <- 0
Delta <- 2 + err2
j <- 1
STOP <- FALSE
Dcurr <- NULL
fcurr <- NULL
if(Gjh >1){
while(!STOP){
j <- j + 1
Dcurr <- Gjh + 2*j - 1 - (j-1)^2*Dprev
Ccurr <- Gjh + 2*j - 1 -(j-1)^2/Cprev
Dcurr <- 1/Dcurr
Delta <- Ccurr * Dcurr
fcurr <- fprev*Delta
fprev <- fcurr
Cprev <- Ccurr
Dprev <- Dcurr
if(abs(Delta - 1)< err2){
STOP <- TRUE
}
}
ret_value <- 1/(Gjh + 1 + fcurr)
}
else{
ret_value <- exp(Gjh)*expint_E1(Gjh)
}
return(ret_value)
}
Lentz_Q(1.2)
Lentz_Q(20202)
Lentz_Q(0.01)
help(xgboost)
library(xgboost)
knitr::opts_chunk$set(echo = TRUE)
library(xgboost)
install.packages("xgboost")
library(xgboost)
help(xgboost)
library(MASS)
library(Matrix)
library(expint)
#source("StandardLDA.R")
#source("DSparseDA.R")
source("VBhorseshoeDA.R")
set.seed(2)
P <- 50; N<-100; k<-6
inds<-rbinom(N,1,0.5)
inds1<-which(inds==1)
inds0<-which(inds==0)
N1<-sum(inds==1)
N0<-sum(inds==0)
combineX<-matrix(0,nrow=N,ncol=P)
sim_mean1<-c(rep(3.5,10),rep(0,P-10))
sim_mean0<-c(rep(0,10),rep(0,P-10))
# sim_mean1<-c(rep(0.7,5),rep(0,P-5))
# sim_SD1<-rep(1,P)
# sim_mean0<-c(rep(0,5),rep(0,P-5))
# sim_SD0<-rep(1,P)
#
# for(i in 1:N1)
# {
#   combineX[inds1[i],]<-rnorm(P,mean=sim_mean1,sd=sim_SD1)
# }
#
# for(i in 1:N0)
# {
#   combineX[inds0[i],]<-rnorm(P,mean=sim_mean0,sd=sim_SD0)
# }
#BLOCK <- as.matrix(0.7^as.matrix(dist(1:10)))
#C = bdiag(BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK,BLOCK)
CLambda <- matrix(0,nrow=P,ncol=k)
for(h in 1:k){
CLambda[sample(1:P,(2*k),replace=FALSE),h] <- rnorm((2*k),mean=0,sd=3)
}
C <- CLambda %*% t(CLambda) + diag(0.5,P)
C
cov2cor(C)
Ccorr <- cov2cor(C)
diag(Ccorr)
Ccorr[13,]
Ccorr[3,]
Ccorr[13,]
sum(abs(Ccorr) >1)
Update_df <- function(vr_alpha, vs_alpha){
p <- length(vr_alpha)
a_psi <- 5
b_psi <- p/2
r_psi <- a_psi + p/2
s_psi <- b_psi - p/2 + 0.5*sum(log(vr_alpha) - digamma(vs_alpha) + vr_alpha/vs_alpha)
return(list(r_psi = r_psi, s_psi = s_psi))
}
Update_df(c(2,3,7,9,20),c(5,2,1,3))
Update_df(c(2,3,7,9,20),c(5,2,10,3))
Update_df <- function(vr_alpha, vs_alpha){
p <- length(vr_alpha)
a_psi <- 5
b_psi <- p/2
r_psi <- a_psi + p/2
browser()
s_psi <- b_psi - p/2 + 0.5*sum(log(vr_alpha) - digamma(vs_alpha) + vr_alpha/vs_alpha)
return(list(r_psi = r_psi, s_psi = s_psi))
}
Update_df(c(2,3,7,9,20),c(5,2,10,3))
b_psi
vr_alpha
vs_alpha
Update_df(c(2,3,7,9,20),c(5,2,10,3,8))
Update_df(rep(1,100),rep(1,100))
Update_df(rep(1,100),rep(3,100))
Update_df(rep(1,100),rep(2,100))
Update_df(rep(1,100),rep(1,100))
Update_df <- function(vr_alpha, vs_alpha){
p <- length(vr_alpha)
a_psi <- 5
b_psi <- p/2
r_psi <- a_psi + p/2
s_psi <- b_psi - p/2 + 0.5*sum(log(vr_alpha) - digamma(vs_alpha) + vr_alpha/vs_alpha)
return(list(r_psi = r_psi, s_psi = s_psi))
}
Update_df(rep(1,100),rep(1,100))
Update_df(rep(1,100),rep(1,100))
Update_df(rep(1,100),rep(2,100))
Update_df(rep(1,100),rep(3,100))
Update_df(rep(5,100),rep(3,100))
Update_df(rep(4,100),rep(3,100))
Update_df(rep(5,100),rep(5,100))
library(tidyverse)
poison_data = read_csv("https://raw.githubusercontent.com/DATA2002/data/master/box_cox_survival.csv")
poison_data = poison_data %>% mutate(inv_survival = 1/y) # create the reciprocal survival time variable
glimpse(poison_data)
poison_sum = poison_data %>% group_by(poison, antidote) %>%
summarise(
mean = mean(inv_survival),
median = median(inv_survival),
sd = sd(inv_survival),
iqr = IQR(inv_survival),
n = n()
)
poison_sum %>% kable(digits = 2)
poison_sum = poison_data %>% group_by(poison, antidote) %>%
summarise(
mean = mean(inv_survival),
median = median(inv_survival),
sd = sd(inv_survival),
iqr = IQR(inv_survival),
n = n()
)
poison_sum %>% kable(digits = 2)
library(knitr)
poison_sum = poison_data %>% group_by(poison, antidote) %>%
summarise(
mean = mean(inv_survival),
median = median(inv_survival),
sd = sd(inv_survival),
iqr = IQR(inv_survival),
n = n()
)
poison_sum %>% kable(digits = 2)
poison_data %>%
ggplot() +
aes(y = inv_survival, x = poison,colour = antidote) +
geom_boxplot() +
theme_bw() +
facet_wrap(~ antidote, ncol = 4) +
labs(y = "1/Survival", x = "Poison", colour = "Antidote")
a1 = aov(inv_survival ~ poison * antidote, data = poison_data)
summary(a1) # could also use anova(a1)
a1 = aov(inv_survival ~ poison * antidote, data = poison_data)
summary(a1) # could also use anova(a1)
library(emmeans)
emmip(a1, antidote ~ poison) + theme_bw()
install.packages("emmeans")
library(emmeans)
emmip(a1, antidote ~ poison) + theme_bw()
library(ggfortify)
autoplot(a1, which = 1:2) + theme_bw()
install.packages("ggfortify")
library(ggfortify)
autoplot(a1, which = 1:2) + theme_bw()
poison_data = poison_data %>%
mutate(
fitted = a1$fitted.values,
resid = a1$residuals
)
d1 = ggplot(poison_data, aes(x = fitted, y = resid)) +
geom_point() + geom_hline(yintercept = 0, colour = "gray", lty = 2) +
theme_bw() + labs(title = "Residuals vs fitted", x = "Fitted values", y = "Residuals")
d2 = ggplot(poison_data, aes(sample = resid)) +
geom_qq() + geom_qq_line() + theme_bw() +
labs(title = "Normal QQ of the residuals", x = "Theoretical quantiles", y = "Residuals")
gridExtra::grid.arrange(d1, d2, ncol = 2)
install.packages("ISLR")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
data(Hitters)
# Remove any samples with missing values
Hitters <- na.omit(Hitters)
selectFeature <- function(train, test, cls.train, cls.test, features) {
## identify a feature to be selected
current.smallest.mse <- Inf
selected.i <- NULL
for(i in 1:ncol(train)) {
current.f <- colnames(train)[i]
# Can't add features that are already in our list
if(current.f %in% c(features, "Salary")) { next }
model <- lm(data=train[,c(features, current.f, "Salary")], Salary ~ .)
# Calculate the mean squared error
test.mse <- mean((cls.test - predict.lm(model, test[,c(features, current.f, "Salary")])) ^ 2)
if(test.mse < current.smallest.mse) {
current.smallest.mse <- test.mse
selected.i <- colnames(train)[i]
}
}
return(selected.i)
}
library(caret)
set.seed(1)
inTrain <- createDataPartition(Hitters$Salary, p = .6)[[1]]
# Remove the column with the Salary
train <- Hitters[ inTrain,]
test  <- Hitters[-inTrain,]
salary.train <- Hitters$Salary[inTrain]
salary.test <- Hitters$Salary[-inTrain]
train
test
features <- c()
current.min.mse <- Inf
for(i in 1:ncol(train)) {
if(colnames(train)[i] == "Salary") { next }
model <- lm(data=train[,c(colnames(train)[i], "Salary")], Salary ~ .)
# Calculate the mean squared error
test.mse <- mean((salary.test - predict.lm(model, test[, c(colnames(train)[i], "Salary")])) ^ 2)
if(test.mse < current.min.mse) {
current.min.mse <- test.mse
features <- colnames(train)[i]
}
}
for (i in 1:4) {
selected.i <- selectFeature(train, test, salary.train, salary.test, features)
print(selected.i)
# add the best feature from current run
features <- c(features, selected.i)
}
features
# Select the first feature
features <- c()
current.min.mse <- Inf
# Find the first feature
for(i in 1:ncol(train)) {
if(colnames(train)[i] == "Salary") { next }
model <- lm(data=train[,c(colnames(train)[i], "Salary")], Salary ~ .)
# Calculate the mean squared error
test.mse <- mean((salary.test - predict.lm(model, test[, c(colnames(train)[i], "Salary")])) ^ 2)
if(test.mse < current.min.mse) {
current.min.mse <- test.mse
features <- colnames(train)[i]
}
}
features
for (i in 1:4) {
selected.i <- selectFeature(train, test, salary.train, salary.test, features)
print(selected.i)
# add the best feature from current run
features <- c(features, selected.i)
}
features
# Top five features selected
print(features)
ridge.mod <- glmnet(x[inTrain,], y[inTrain], alpha=0, lambda=grid, standardize = TRUE)
library(glmnet)
set.seed(1)
inTrain <- createDataPartition(Hitters$Salary, p = .6)[[1]]
x <- model.matrix(Salary ~ ., Hitters)[,-1]
y <- Hitters$Salary
# set the range of lambda values to be tested.
grid <- 10^seq(8,-2, length=100)
ridge.mod <- glmnet(x[inTrain,], y[inTrain], alpha=0, lambda=grid, standardize = TRUE)
plot(ridge.mod, xvar="lambda", label=TRUE)
# Using the inbuilt function to do cross-validation
set.seed(1)
cv.out <- cv.glmnet(x[inTrain,], y[inTrain], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
# Use the best CV model to predict the test set Salary
test.pred <- predict(cv.out, x[-inTrain,])[,1]
mse <- mean((test.pred - y[-inTrain])^2)
mse
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
hubble = read_tsv("https://raw.githubusercontent.com/DATA2002/data/master/Hubble.txt")
glimpse(hubble)
hubble$recession_velocity
hubble$distance
install.packages("stargazer")
library(stargazer)
# stargazer(hfit1, hfit2, type = 'latex', header = FALSE)
stargazer(hfit1, hfit2, type = "html")
# stargazer(hfit1, hfit2, type = 'latex', header = FALSE)
stargazer(hfit1, hfit2, type = "html")
install.packages("sjPlot")
install.packages("sjPlot")
install.packages("GGally")
install.packages(c("d3heatmap", "pairsD3", "skimr"))
install.packages("mplot")
install.packages("tibble")
install.packages("tibble")
install.packages("tibble")
install.packages("tibble")
install.packages("tibble")
install.packages("pillar")
install.packages("pillar")
knitr::opts_chunk$set(echo = TRUE)
Props <- c(100,500,900,1200)/2700
P <- matrix(0,nrow=4,ncol=4)
for(i in 1:4){
for(j in 1:4){
P[i,j] = Props[j]
}
}
matrix(Props,nrow=1) %*% P - matrix(Props,nrow=1)
VisitNum = 50000
VisitLocation = 1
Locations = 1:4
sample(Locations, 1, prob=P[VisitLocation[1],])
sample(Locations, 1, prob=P[VisitLocation[1],])
sample(Locations, 1, prob=P[VisitLocation[1],])
sample(Locations, 1, prob=P[VisitLocation[1],])
matrix(Props,nrow=1) %*% P - matrix(Props,nrow=1)
VisitNum = 50000
VisitLocation = 1
Locations = 1:4
set.seed(123)
for(t in 2:VisitNum){
VisitLocation[t] = sample(Locations, 1, prob=P[VisitLocation[t-1],])
}
VisitLocation
table(VisitLocation)
table(VisitLocation)/50000
abs((table(VisitLocation)/50000) - Props)
P %*% P
P %*% P %*% P
p <- 0.4
mu <- c(-1, 2)
sd <- c(.2, 1.5)
f <- function(x) {
p     * dnorm(x, mu[1], sd[1]) +
(1-p) * dnorm(x, mu[2], sd[2])
}
curve(f(x), col="red", -4, 8, n=300, las=1)
M = 51000
moves = rnorm(1,0,20)
for(t in 1:M){
propose.x = moves[t] + 5*rnorm(1)
current.x = moves[t]
alpha.prop = min(1, exp(log(f(propose.x)) - log(f(current.x))))
if(alpha.prop <= 1){
coin = rbinom(1,1,prob=alpha.prop)
if(coin==1){
moves[t+1] = propose.x
}
else{
moves[t+1] = current.x
}
}
else{
moves[t+1] = propose.x
}
}
moves
moves[1001:51000]
plot(density(moves[1001:51000]))
M = 110000
moves = rnorm(1,0,20)
for(t in 1:M){
propose.x = moves[t] + 5*rnorm(1)
current.x = moves[t]
alpha.prop = min(1, exp(log(f(propose.x)) - log(f(current.x))))
if(alpha.prop <= 1){
coin = rbinom(1,1,prob=alpha.prop)
if(coin==1){
moves[t+1] = propose.x
}
else{
moves[t+1] = current.x
}
}
else{
moves[t+1] = propose.x
}
}
plot(density(moves[10001:110000]))
library(rstan)
25*8
25*4
25*5
20/500
library(xaringan)
xaringan:inf_mr()
xaringan::inf_mr()
getwd()
getwd()
lgamma(0.5)
digamma(0)
V <- list(matrix(rt(100000,df=2),nrow=100))
for(j in 1:100){
V[[j]] <- matrix(rt(100000,df=2),nrow=100)
}
OutputList <- mclapply(V, function(s){ sample(colMeans(s),1)  },mc.cores = 4)
OutputList <- mclapply(V, function(s){ sample(colMeans(s),1)  },mc.cores = 4)
library(parallel)
OutputList <- mclapply(V, function(s){ sample(colMeans(s),1)  },mc.cores = 4)
print(OutputList)
4/(4-2)
3/(3-2)
student.id <- 1:100
TeacherA <- runif(100,0,100)
TeacherB <- runif(100,0,100)
dataStudents <- data.frame(student.id,TeacherA,TeacherB)
dataStudents
head(dataStudents)
colMeans(dataStudents)
t.test(TeacherA,TeacherB,equal.variance=TRUE)
help("t.test")
t.test(TeacherA,TeacherB,var.equal=TRUE)
t.test(dataStudents$TeacherA - dataStudents$TeacherB, mu=0)
var(dataStudents$TeacherA)
dataStudents$TeacherA
mean(dataStudents$TeacherA)
var(dataStudents$TeacherA)
curve(dchisq(x,df=3))
curve(dchisq(x,df=3),xlim=c(0,10))
help("t.test")
B <- 10000
Bootstrap.store <- rep(0,B)
for(t in 1:B){
xA <- sample(dataStudents$TeacherA,100,replace=TRUE)
xB <- sample(dataStudents$TeacherB,100,replace=TRUE)
Bootstrap.store <- t.test(xA,xB,var.equal = TRUE)$statistic
}
boxplot(Bootstrap.store)
B <- 10000
Bootstrap.store <- rep(0,B)
for(t in 1:B){
xA <- sample(dataStudents$TeacherA,100,replace=TRUE)
xB <- sample(dataStudents$TeacherB,100,replace=TRUE)
Bootstrap.store[t] <- t.test(xA,xB,var.equal = TRUE)$statistic
}
boxplot(Bootstrap.store)
plot(density(Bootstrap.store))
actual.stat <- t.test(dataStudents$TeacherA,dataStudents$TeacherB,var.equal=TRUE)
actual.stat
actual.stat <- t.test(dataStudents$TeacherA,dataStudents$TeacherB,var.equal=TRUE)$statistic
actual.stat
sum(Bootstrap.store <= actual.stat) + sum(Bootstrap.store <= abs(actual.stat))
length(Bootstrap.store)
sum(Bootstrap.store <= actual.stat)
mean(Bootstrap.store <= actual.stat)
mean(abs(Bootstrap.store) >= actual.stat)
Bootstrap.store
mean(abs(Bootstrap.store) >= abs(actual.stat))
mean(Bootstrap.store <= actual.stat)
sum(Bootstrap.store <= actual.stat)
mean(abs(Bootstrap.store) >= abs(actual.stat))
load("~/Dropbox (Sydney Uni)/EmpiricalLikelihood/Codes2/SLRresults.RData")
HMCslrv$acceptance.rate
dim(HMCslrv$samples)
var(HMCslrv$samples[501:1000,])
slrvTest2$CList[[1]]
length(slrvTest2$CList)
VarList <- lapply(slrvTest2$CList[7501:17500],function(m){ m %*% t(m) })
SVBcov <- matrix(colMeans(matrix(unlist(VarList),ncol=9,byrow=T)),byrow=T,ncol=3)
SVBmean <- colMeans(slrvTest$muMAT[7501:17500,])
SVBcov
var(HMCslrv$samples[501:1000,])
var(HMCslrv$samples[101:1000,])
SVBmean <- colMeans(slrvTest2$muMAT[7501:17500,])
SVBmean
colMeans(HMCslrv$samples[101:1000,])
length(slrvTest$CList)
SVBmean <- colMeans(slrvTest2$muMAT[17501:27500,])
SVBmean <- colMeans(slrvTest$muMAT[17501:27500,])
SVBmean
VarList <- lapply(slrvTest$CList[17501:27500],function(m){ m %*% t(m) })
SVBcov <- matrix(colMeans(matrix(unlist(VarList),ncol=9,byrow=T)),byrow=T,ncol=3)
SVBcov
var(HMCslrv$samples[101:1000,])
load("~/Dropbox (Sydney Uni)/EmpiricalLikelihood/Codes2/ARCHhmcsample.RData")
setwd("~/Desktop/Teaching/MAST30034Github/MAST30034Repo/Codes")
